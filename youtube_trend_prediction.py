# -*- coding: utf-8 -*-
"""Youtube Trend Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j6G-nllINVbdFo8hnk-GcTwQI1IWvYaq
"""

pip install textblob

from googleapiclient.discovery import build

# Set up your API key
api_key = 'AIzaSyA4yAQQMtHy-F1-MZAL_G51ia3Ag-TxcQg'
youtube = build('youtube', 'v3', developerKey=api_key)

# Search for trending videos
trending_videos = youtube.videos().list(
    chart='mostPopular',
    regionCode='US',
    part='snippet',
    maxResults=50  # Adjust as needed
).execute()
import csv

with open('trending_videos.csv', 'w', newline='') as csvfile:
    fieldnames = ['video_id', 'title', 'description']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()
    for video in trending_videos['items']:
        writer.writerow({
            'video_id': video['id'],
            'title': video['snippet']['title'],
            'description': video['snippet']['description']
        })

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

data = pd.read_csv('trending_videos.csv')
data.head()

from textblob import TextBlob
# Define the calculate_sentiment function
def calculate_sentiment(text):
    analysis = TextBlob(text)
    # Assign a label based on sentiment
    if analysis.sentiment.polarity >= 0:
        return 1  # Positive sentiment, consider it trending
    else:
        return 0  # Negative sentiment, consider it not trending
data['trend_label'] = data.apply(lambda row: calculate_sentiment(row['title'] + ' ' + row['description']), axis=1)

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text
data['video_id'] = data['video_id'].apply(preprocess_text)
data['title'] = data['title'].apply(preprocess_text)
data['description'] = data['description'].apply(preprocess_text)

data.head()

X_train, X_test, y_train, y_test = train_test_split(data['title'] + ' ' + data['description'], data['trend_label'], test_size=0.2, random_state=42)

max_words = 10000  # Define the maximum number of words to keep

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

maxlen = 100
X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen)
X_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen)

model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=100, input_length=maxlen))
model.add(LSTM(128))
model.add(Dense(123, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(34, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_padded, y_train, epochs=10, batch_size=64, validation_data=(X_test_padded, y_test))

loss, accuracy = model.evaluate(X_test_padded, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

new_text = ["Miruthan - Munnal Kadhali Video | Jayam Ravi | D. Imman", "Watch Munnal Kadhali official video from the movie Miruthan Song Name - Munnal Kadhali"]
new_text_sequence = tokenizer.texts_to_sequences(new_text)
new_text_padded = pad_sequences(new_text_sequence, maxlen=maxlen)
prediction = model.predict(new_text_padded)

prediction

model.save('/content/drive/MyDrive/Assignment/DL Assignment 2/youtube_model.h5')

from google.colab import drive
drive.mount('/content/drive')